# AI-Safety
Contains all the papers presented in ACM Summer School on Generative AI for Text 2024


## Identifying harmful behaviour of language models

- ğŸ¯ Somnath Banerjee, Sayan Layek, __Rima Hazra__, Animesh Mukherjee. __How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries__. ğŸ‘‰ [Paper](https://arxiv.org/abs/2402.15302) [Under Review]
  
- Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran. __ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs__ ğŸ‘‰ [Paper](https://arxiv.org/abs/2402.11753) [ACL 2024]
  
- Divij Handa, Advait Chirmule, Bimal Gajera, Chitta Baral. __Jailbreaking Proprietary Large Language Models using Word Substitution Cipher__. ğŸ‘‰ [Paper](https://arxiv.org/abs/2402.10601) [Under Review]
  
- Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing. __Multilingual Jailbreak Challenges in Large Language Models__. ğŸ‘‰ [Paper](https://arxiv.org/abs/2310.06474) [ICLR 2024]
  
- Javier Rando, Florian TramÃ¨r. __Universal Jailbreak Backdoors from Poisoned Human Feedback__. ğŸ‘‰ [Paper](https://arxiv.org/abs/2311.14455) [ICLR 2024]
  
- ğŸ¯ __Rima Hazra__, Sayan Layek, Somnath Banerjee, Soujanya Poria. __Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models__. ğŸ‘‰ [Paper](https://arxiv.org/abs/2401.10647) [ACL 2024]
  
- Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson. __Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!__. ğŸ‘‰ [Paper](https://arxiv.org/abs/2310.03693) [ICLR 2024]
  
- ğŸ¯ __Rima Hazra__, Sayan Layek, Somnath Banerjee, Soujanya Poria. __Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations__. ğŸ‘‰ [Paper](https://arxiv.org/abs/2406.11801v1) [Under Review]
  
- ğŸ¯ Somnath Banerjee, Soham Tripathy, Sayan Layek, Shanu Kumar, Animesh Mukherjee, __Rima Hazra__. __SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large Language Models__. ğŸ‘‰ [Paper](https://arxiv.org/abs/2406.12274) [Under Review]


## Safety evaluation datasets

- NicheHazardQA ğŸ‘‰ [download](https://huggingface.co/datasets/SoftMINER-Group/NicheHazardQA) ğŸ¯
- TechHazardQA ğŸ‘‰ [download](https://huggingface.co/datasets/SoftMINER-Group/TechHazardQA) ğŸ¯
- DangerousQA ğŸ‘‰ [download](https://github.com/SALT-NLP/chain-of-thought-bias/tree/main?tab=readme-ov-file)
- AdvBench ğŸ‘‰ [download](https://huggingface.co/datasets/kelly8tom/advbench_orig)
- Anthropic HH dataset ğŸ‘‰ [download](https://huggingface.co/datasets/Anthropic/hh-rlhf)


## Demo codebase

- Uploadiing the codes soon.

## Support

- â­ï¸ If you find the github resources helpful, our papers and datasets (ğŸ¯) interesting, <b>please encourage us by starring, upvoting and sharing our papers and datasets!</b> ğŸ˜Š
